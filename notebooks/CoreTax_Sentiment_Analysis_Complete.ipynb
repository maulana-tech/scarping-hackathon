{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìä CoreTax Sentiment Analysis - Complete Pipeline\n",
                "## Analisis Sentimen Data Combined dari Semua Platform\n",
                "\n",
                "**Data Source:** `CoreTax Combined Data Clean.csv`\n",
                "\n",
                "**Pipeline:**\n",
                "1. ‚úÖ Load & Explore Data\n",
                "2. ‚úÖ Text Preprocessing (with visualization)\n",
                "3. ‚úÖ Sentiment Labeling (RoBERTa Model)\n",
                "4. ‚úÖ Sentiment Analysis & Visualization\n",
                "5. ‚úÖ Keyword Extraction (TF-IDF & IndoBERT)\n",
                "6. ‚úÖ WordCloud Generation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup & Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "print(\"Mounting Google Drive...\")\n",
                "drive.mount('/content/drive/')\n",
                "print(\"‚úì Google Drive mounted!\")\n",
                "\n",
                "# Set data path\n",
                "DATA_PATH = '/content/drive/MyDrive/Hackathon/data-actual/'\n",
                "FILE_NAME = 'CoreTax Combined Data Clean.csv'\n",
                "\n",
                "print(f\"\\nData path: {DATA_PATH}\")\n",
                "print(f\"File name: {FILE_NAME}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q transformers torch Sastrawi wordcloud scikit-learn matplotlib seaborn pandas numpy tqdm sentence-transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import warnings\n",
                "import torch\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# NLP Libraries\n",
                "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
                "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
                "from transformers import pipeline, AutoTokenizer, AutoModel\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from wordcloud import WordCloud\n",
                "\n",
                "# TF-IDF & Clustering\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "\n",
                "# Progress bar\n",
                "from tqdm import tqdm\n",
                "tqdm.pandas()\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette(\"husl\")\n",
                "\n",
                "print(\"‚úì All libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Load & Explore Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"LOADING DATA\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Load data\n",
                "df = pd.read_csv(DATA_PATH + FILE_NAME, encoding='utf-8')\n",
                "\n",
                "print(f\"\\n‚úì Data loaded successfully!\")\n",
                "print(f\"Total rows: {len(df):,}\")\n",
                "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
                "print(f\"\\nData shape: {df.shape}\")\n",
                "\n",
                "# Display first few rows\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"PREVIEW DATA\")\n",
                "print(\"=\" * 80)\n",
                "display(df.head(10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data info\n",
                "print(\"=\" * 80)\n",
                "print(\"DATA INFORMATION\")\n",
                "print(\"=\" * 80)\n",
                "df.info()\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"MISSING VALUES\")\n",
                "print(\"=\" * 80)\n",
                "print(df.isnull().sum())\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"DISTRIBUTION BY SOURCE\")\n",
                "print(\"=\" * 80)\n",
                "print(df['source'].value_counts())\n",
                "\n",
                "# Visualize source distribution\n",
                "plt.figure(figsize=(10, 6))\n",
                "df['source'].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')\n",
                "plt.title('Distribution of Data by Source', fontsize=16, fontweight='bold')\n",
                "plt.xlabel('Source', fontsize=12)\n",
                "plt.ylabel('Count', fontsize=12)\n",
                "plt.xticks(rotation=45)\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Text Preprocessing\n",
                "\n",
                "### Preprocessing Steps:\n",
                "1. Lowercase conversion\n",
                "2. Remove URLs, mentions, emails\n",
                "3. Remove hashtags (keep words)\n",
                "4. Remove special characters & numbers\n",
                "5. Normalize slang words\n",
                "6. Remove stopwords\n",
                "7. Stemming"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Slang dictionary (Indonesian)\n",
                "slang_dict = {\n",
                "    'gak': 'tidak', 'ga': 'tidak', 'gk': 'tidak', 'ngga': 'tidak', 'nggak': 'tidak',\n",
                "    'udah': 'sudah', 'udh': 'sudah', 'dah': 'sudah',\n",
                "    'aja': 'saja', 'aj': 'saja',\n",
                "    'banget': 'sangat', 'bgt': 'sangat',\n",
                "    'emang': 'memang', 'emg': 'memang',\n",
                "    'gimana': 'bagaimana', 'gmn': 'bagaimana', 'gmana': 'bagaimana',\n",
                "    'kenapa': 'mengapa', 'knp': 'mengapa', 'knapa': 'mengapa',\n",
                "    'kok': 'mengapa',\n",
                "    'dong': '', 'sih': '',\n",
                "    'nih': 'ini', 'tuh': 'itu',\n",
                "    'yg': 'yang', 'dgn': 'dengan', 'utk': 'untuk', 'krn': 'karena',\n",
                "    'klo': 'kalau', 'kalo': 'kalau',\n",
                "    'tp': 'tapi', 'tpi': 'tapi',\n",
                "    'jd': 'jadi', 'jdi': 'jadi',\n",
                "    'lg': 'lagi', 'lgi': 'lagi',\n",
                "    'sm': 'sama', 'dr': 'dari', 'dri': 'dari',\n",
                "    'sdh': 'sudah', 'blm': 'belum', 'blum': 'belum',\n",
                "    'hrs': 'harus', 'trs': 'terus', 'trus': 'terus',\n",
                "    'bisa': 'bisa', 'bs': 'bisa',\n",
                "    'cuma': 'hanya', 'cm': 'hanya',\n",
                "    'org': 'orang',\n",
                "    'skrg': 'sekarang', 'skr': 'sekarang',\n",
                "    'bgm': 'bagaimana', 'dmn': 'dimana',\n",
                "    'mksh': 'terima kasih', 'makasih': 'terima kasih', 'thx': 'terima kasih',\n",
                "    'pls': 'tolong', 'plz': 'tolong', 'plis': 'tolong',\n",
                "    'wkwk': '', 'wkwkwk': '', 'haha': '', 'hehe': '', 'hihi': '',\n",
                "    'anjir': 'jelek', 'anjing': 'jelek', 'anj': 'jelek',\n",
                "    'bangsat': 'jelek', 'bngst': 'jelek',\n",
                "    'tai': 'jelek', 'taik': 'jelek',\n",
                "    'ampas': 'jelek', 'sampah': 'jelek',\n",
                "    'eror': 'error', 'erorr': 'error', 'erorrr': 'error',\n",
                "    'lemot': 'lambat', 'lelet': 'lambat',\n",
                "    'ribet': 'rumit', 'ruwet': 'rumit',\n",
                "    'susah': 'sulit', 'gampang': 'mudah',\n",
                "    'keren': 'bagus', 'mantap': 'bagus', 'mantul': 'bagus',\n",
                "    'jelek': 'buruk', 'parah': 'buruk', 'payah': 'buruk',\n",
                "    'bagus': 'baik', 'oke': 'baik', 'ok': 'baik',\n",
                "}\n",
                "\n",
                "# Custom stopwords\n",
                "custom_stopwords = [\n",
                "    'coretax', 'djp', 'pajak', 'npwp', 'kpp', 'aplikasi', 'website', 'web', 'app',\n",
                "    'bang', 'kak', 'min', 'pak', 'bu', 'mas', 'mbak',\n",
                "    'yg', 'nya', 'nih', 'tuh', 'dong', 'sih', 'deh', 'lah', 'kok',\n",
                "]\n",
                "\n",
                "print(\"‚úì Slang dictionary and custom stopwords loaded!\")\n",
                "print(f\"  - Slang words: {len(slang_dict)}\")\n",
                "print(f\"  - Custom stopwords: {len(custom_stopwords)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Sastrawi\n",
                "print(\"Initializing Sastrawi...\")\n",
                "stemmer_factory = StemmerFactory()\n",
                "stemmer = stemmer_factory.create_stemmer()\n",
                "\n",
                "stopword_factory = StopWordRemoverFactory()\n",
                "stopwords = stopword_factory.get_stop_words()\n",
                "stopwords.extend(custom_stopwords)\n",
                "\n",
                "print(f\"‚úì Sastrawi initialized!\")\n",
                "print(f\"  - Total stopwords: {len(stopwords)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text):\n",
                "    if pd.isna(text) or text == '':\n",
                "        return \"\"\n",
                "    text = str(text).lower()\n",
                "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
                "    text = re.sub(r'@\\w+', '', text)\n",
                "    text = re.sub(r'\\S+@\\S+', '', text)\n",
                "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
                "    text = re.sub(r'\\d+', '', text)\n",
                "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    return text\n",
                "\n",
                "def normalize_slang(text):\n",
                "    words = text.split()\n",
                "    normalized = [slang_dict.get(word, word) for word in words]\n",
                "    return ' '.join(normalized)\n",
                "\n",
                "def remove_stopwords(text):\n",
                "    words = text.split()\n",
                "    filtered = [word for word in words if word not in stopwords and len(word) > 2]\n",
                "    return ' '.join(filtered)\n",
                "\n",
                "def stem_text(text):\n",
                "    return stemmer.stem(text)\n",
                "\n",
                "print(\"‚úì Preprocessing functions defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply preprocessing\n",
                "print(\"=\" * 80)\n",
                "print(\"APPLYING PREPROCESSING\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Use 'text' column if 'cleaned_text' doesn't exist or is empty\n",
                "if 'cleaned_text' not in df.columns or df['cleaned_text'].isna().all():\n",
                "    text_column = 'text'\n",
                "else:\n",
                "    text_column = 'cleaned_text'\n",
                "\n",
                "print(f\"\\nUsing column: {text_column}\")\n",
                "df['original_text'] = df[text_column].copy()\n",
                "\n",
                "print(\"\\n1. Cleaning text...\")\n",
                "df['preprocessed'] = df[text_column].progress_apply(clean_text)\n",
                "\n",
                "print(\"\\n2. Normalizing slang...\")\n",
                "df['preprocessed'] = df['preprocessed'].progress_apply(normalize_slang)\n",
                "\n",
                "print(\"\\n3. Removing stopwords...\")\n",
                "df['preprocessed'] = df['preprocessed'].progress_apply(remove_stopwords)\n",
                "\n",
                "print(\"\\n4. Applying stemming...\")\n",
                "df['preprocessed'] = df['preprocessed'].progress_apply(stem_text)\n",
                "\n",
                "# Remove empty texts\n",
                "df = df[df['preprocessed'].str.len() > 0].reset_index(drop=True)\n",
                "\n",
                "print(f\"\\n‚úì Preprocessing complete!\")\n",
                "print(f\"Total data after preprocessing: {len(df):,} rows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üìä Visualization: Before vs After Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample comparison\n",
                "print(\"=\" * 80)\n",
                "print(\"BEFORE vs AFTER PREPROCESSING - SAMPLE COMPARISON\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "sample_df = df.sample(n=10, random_state=42)[['original_text', 'preprocessed', 'source']]\n",
                "sample_df.index = range(1, 11)\n",
                "\n",
                "for idx, row in sample_df.iterrows():\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"Sample #{idx} | Source: {row['source']}\")\n",
                "    print(f\"{'='*80}\")\n",
                "    print(f\"BEFORE: {row['original_text'][:200]}...\")\n",
                "    print(f\"\\nAFTER:  {row['preprocessed'][:200]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Text length comparison\n",
                "df['original_length'] = df['original_text'].str.len()\n",
                "df['preprocessed_length'] = df['preprocessed'].str.len()\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Before preprocessing\n",
                "axes[0].hist(df['original_length'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
                "axes[0].set_title('Text Length Distribution - BEFORE Preprocessing', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Character Count', fontsize=12)\n",
                "axes[0].set_ylabel('Frequency', fontsize=12)\n",
                "axes[0].axvline(df['original_length'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[\"original_length\"].mean():.0f}')\n",
                "axes[0].legend()\n",
                "axes[0].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# After preprocessing\n",
                "axes[1].hist(df['preprocessed_length'], bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
                "axes[1].set_title('Text Length Distribution - AFTER Preprocessing', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Character Count', fontsize=12)\n",
                "axes[1].set_ylabel('Frequency', fontsize=12)\n",
                "axes[1].axvline(df['preprocessed_length'].mean(), color='green', linestyle='--', linewidth=2, label=f'Mean: {df[\"preprocessed_length\"].mean():.0f}')\n",
                "axes[1].legend()\n",
                "axes[1].grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nStatistics:\")\n",
                "print(f\"  BEFORE - Mean: {df['original_length'].mean():.2f}, Median: {df['original_length'].median():.2f}\")\n",
                "print(f\"  AFTER  - Mean: {df['preprocessed_length'].mean():.2f}, Median: {df['preprocessed_length'].median():.2f}\")\n",
                "print(f\"  Reduction: {((df['original_length'].mean() - df['preprocessed_length'].mean()) / df['original_length'].mean() * 100):.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Sentiment Labeling with RoBERTa\n",
                "\n",
                "Using: `w11wo/indonesian-roberta-base-sentiment-classifier`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize RoBERTa sentiment classifier\n",
                "print(\"=\" * 80)\n",
                "print(\"INITIALIZING ROBERTA SENTIMENT CLASSIFIER\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(\"\\nLoading model: w11wo/indonesian-roberta-base-sentiment-classifier\")\n",
                "print(\"This may take a few minutes...\\n\")\n",
                "\n",
                "sentiment_classifier = pipeline(\n",
                "    \"sentiment-analysis\",\n",
                "    model=\"w11wo/indonesian-roberta-base-sentiment-classifier\",\n",
                "    device=0 if torch.cuda.is_available() else -1\n",
                ")\n",
                "\n",
                "print(\"\\n‚úì RoBERTa model loaded successfully!\")\n",
                "print(f\"  - Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_sentiment_roberta(text):\n",
                "    if pd.isna(text) or text == '':\n",
                "        return 'neutral', 0.0\n",
                "    \n",
                "    try:\n",
                "        text = text[:512]\n",
                "        result = sentiment_classifier(text)[0]\n",
                "        \n",
                "        label_map = {\n",
                "            'LABEL_0': 'negative',\n",
                "            'LABEL_1': 'neutral',\n",
                "            'LABEL_2': 'positive',\n",
                "            'negative': 'negative',\n",
                "            'neutral': 'neutral',\n",
                "            'positive': 'positive'\n",
                "        }\n",
                "        \n",
                "        sentiment = label_map.get(result['label'], 'neutral')\n",
                "        score = result['score']\n",
                "        \n",
                "        return sentiment, score\n",
                "    except Exception as e:\n",
                "        print(f\"Error processing text: {e}\")\n",
                "        return 'neutral', 0.0\n",
                "\n",
                "print(\"‚úì Sentiment function defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply sentiment labeling\n",
                "print(\"=\" * 80)\n",
                "print(\"APPLYING SENTIMENT LABELING\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(f\"\\nProcessing {len(df):,} texts...\")\n",
                "print(\"This may take a while depending on your hardware...\\n\")\n",
                "\n",
                "tqdm.pandas(desc=\"Analyzing sentiment\")\n",
                "df[['sentiment_roberta', 'sentiment_score_roberta']] = df['original_text'].progress_apply(\n",
                "    lambda x: pd.Series(get_sentiment_roberta(x))\n",
                ")\n",
                "\n",
                "print(f\"\\n‚úì Sentiment labeling complete!\")\n",
                "print(f\"\\nSentiment distribution:\")\n",
                "print(df['sentiment_roberta'].value_counts())\n",
                "print(f\"\\nAverage confidence score: {df['sentiment_score_roberta'].mean():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Sentiment Analysis & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Overall sentiment distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "sentiment_counts = df['sentiment_roberta'].value_counts()\n",
                "colors = {'positive': '#2ecc71', 'neutral': '#f39c12', 'negative': '#e74c3c'}\n",
                "bar_colors = [colors.get(sent, 'gray') for sent in sentiment_counts.index]\n",
                "\n",
                "axes[0].bar(sentiment_counts.index, sentiment_counts.values, color=bar_colors, edgecolor='black', alpha=0.8)\n",
                "axes[0].set_title('Overall Sentiment Distribution', fontsize=16, fontweight='bold')\n",
                "axes[0].set_xlabel('Sentiment', fontsize=12)\n",
                "axes[0].set_ylabel('Count', fontsize=12)\n",
                "axes[0].grid(axis='y', alpha=0.3)\n",
                "\n",
                "for i, (sent, count) in enumerate(sentiment_counts.items()):\n",
                "    axes[0].text(i, count, f'{count:,}\\n({count/len(df)*100:.1f}%)', \n",
                "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
                "\n",
                "axes[1].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
                "           colors=[colors.get(sent, 'gray') for sent in sentiment_counts.index],\n",
                "           startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
                "axes[1].set_title('Sentiment Proportion', fontsize=16, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sentiment by source\n",
                "sentiment_by_source = pd.crosstab(df['source'], df['sentiment_roberta'], normalize='index') * 100\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(14, 8))\n",
                "sentiment_by_source.plot(kind='bar', ax=ax, color=[colors.get(col, 'gray') for col in sentiment_by_source.columns],\n",
                "                         edgecolor='black', alpha=0.8)\n",
                "ax.set_title('Sentiment Distribution by Source (%)', fontsize=16, fontweight='bold')\n",
                "ax.set_xlabel('Source', fontsize=12)\n",
                "ax.set_ylabel('Percentage (%)', fontsize=12)\n",
                "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
                "ax.legend(title='Sentiment', title_fontsize=12, fontsize=11)\n",
                "ax.grid(axis='y', alpha=0.3)\n",
                "\n",
                "for container in ax.containers:\n",
                "    ax.bar_label(container, fmt='%.1f%%', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nSentiment distribution by source (count):\")\n",
                "print(pd.crosstab(df['source'], df['sentiment_roberta']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confidence score distribution\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "for idx, sentiment in enumerate(['positive', 'neutral', 'negative']):\n",
                "    data = df[df['sentiment_roberta'] == sentiment]['sentiment_score_roberta']\n",
                "    \n",
                "    axes[idx].hist(data, bins=30, color=colors[sentiment], edgecolor='black', alpha=0.7)\n",
                "    axes[idx].set_title(f'{sentiment.capitalize()} - Confidence Score Distribution', \n",
                "                       fontsize=14, fontweight='bold')\n",
                "    axes[idx].set_xlabel('Confidence Score', fontsize=11)\n",
                "    axes[idx].set_ylabel('Frequency', fontsize=11)\n",
                "    axes[idx].axvline(data.mean(), color='red', linestyle='--', linewidth=2, \n",
                "                     label=f'Mean: {data.mean():.3f}')\n",
                "    axes[idx].legend()\n",
                "    axes[idx].grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Keyword Extraction\n",
                "\n",
                "### Method 1: TF-IDF (Statistical Approach)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"KEYWORD EXTRACTION USING TF-IDF\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "def extract_keywords_tfidf(texts, top_n=20):\n",
                "    vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
                "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
                "    \n",
                "    feature_names = vectorizer.get_feature_names_out()\n",
                "    tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
                "    \n",
                "    keywords_df = pd.DataFrame({\n",
                "        'keyword': feature_names,\n",
                "        'tfidf_score': tfidf_scores\n",
                "    }).sort_values('tfidf_score', ascending=False).head(top_n)\n",
                "    \n",
                "    return keywords_df\n",
                "\n",
                "# Extract keywords for each sentiment\n",
                "for sentiment in ['positive', 'neutral', 'negative']:\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"Top Keywords (TF-IDF) - {sentiment.upper()}\")\n",
                "    print(f\"{'='*80}\")\n",
                "    \n",
                "    texts = df[df['sentiment_roberta'] == sentiment]['preprocessed'].tolist()\n",
                "    \n",
                "    if len(texts) > 0:\n",
                "        keywords = extract_keywords_tfidf(texts, top_n=20)\n",
                "        print(keywords.to_string(index=False))\n",
                "        \n",
                "        plt.figure(figsize=(12, 6))\n",
                "        plt.barh(keywords['keyword'], keywords['tfidf_score'], color=colors[sentiment], edgecolor='black', alpha=0.8)\n",
                "        plt.xlabel('TF-IDF Score', fontsize=12)\n",
                "        plt.ylabel('Keyword', fontsize=12)\n",
                "        plt.title(f'Top 20 Keywords (TF-IDF) - {sentiment.capitalize()} Sentiment', fontsize=14, fontweight='bold')\n",
                "        plt.gca().invert_yaxis()\n",
                "        plt.grid(axis='x', alpha=0.3)\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "    else:\n",
                "        print(f\"No data for {sentiment} sentiment\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Method 2: IndoBERT (Contextual Approach)\n",
                "\n",
                "Using: `indobenchmark/indobert-base-p1`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize IndoBERT\n",
                "print(\"=\" * 80)\n",
                "print(\"INITIALIZING INDOBERT FOR KEYWORD EXTRACTION\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(\"\\nLoading model: indobenchmark/indobert-base-p1\")\n",
                "print(\"This may take a few minutes...\\n\")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
                "model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
                "\n",
                "# Move to GPU if available\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = model.to(device)\n",
                "model.eval()\n",
                "\n",
                "print(\"\\n‚úì IndoBERT model loaded successfully!\")\n",
                "print(f\"  - Device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_bert_embeddings(texts, batch_size=32):\n",
                "    \"\"\"\n",
                "    Get BERT embeddings for texts\n",
                "    \"\"\"\n",
                "    embeddings = []\n",
                "    \n",
                "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
                "        batch_texts = texts[i:i+batch_size]\n",
                "        \n",
                "        # Tokenize\n",
                "        encoded = tokenizer(batch_texts, padding=True, truncation=True, \n",
                "                          max_length=512, return_tensors='pt')\n",
                "        \n",
                "        # Move to device\n",
                "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
                "        \n",
                "        # Get embeddings\n",
                "        with torch.no_grad():\n",
                "            outputs = model(**encoded)\n",
                "            # Use [CLS] token embedding\n",
                "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
                "            embeddings.extend(batch_embeddings)\n",
                "    \n",
                "    return np.array(embeddings)\n",
                "\n",
                "def extract_keywords_bert(texts, top_n=20, sample_size=1000):\n",
                "    \"\"\"\n",
                "    Extract keywords using BERT embeddings and cosine similarity\n",
                "    \"\"\"\n",
                "    # Sample texts if too many\n",
                "    if len(texts) > sample_size:\n",
                "        sample_texts = np.random.choice(texts, sample_size, replace=False).tolist()\n",
                "    else:\n",
                "        sample_texts = texts\n",
                "    \n",
                "    # Get all unique words\n",
                "    all_words = []\n",
                "    for text in sample_texts:\n",
                "        all_words.extend(text.split())\n",
                "    \n",
                "    # Count word frequencies\n",
                "    from collections import Counter\n",
                "    word_freq = Counter(all_words)\n",
                "    \n",
                "    # Get top words by frequency (candidates)\n",
                "    top_words = [word for word, _ in word_freq.most_common(100)]\n",
                "    \n",
                "    # Get embeddings for documents and words\n",
                "    print(f\"  Processing {len(sample_texts)} documents...\")\n",
                "    doc_embeddings = get_bert_embeddings(sample_texts)\n",
                "    \n",
                "    print(f\"  Processing {len(top_words)} candidate keywords...\")\n",
                "    word_embeddings = get_bert_embeddings(top_words)\n",
                "    \n",
                "    # Calculate average document embedding\n",
                "    avg_doc_embedding = doc_embeddings.mean(axis=0).reshape(1, -1)\n",
                "    \n",
                "    # Calculate similarity between words and average document\n",
                "    similarities = cosine_similarity(word_embeddings, avg_doc_embedding).flatten()\n",
                "    \n",
                "    # Create dataframe\n",
                "    keywords_df = pd.DataFrame({\n",
                "        'keyword': top_words,\n",
                "        'bert_score': similarities,\n",
                "        'frequency': [word_freq[word] for word in top_words]\n",
                "    }).sort_values('bert_score', ascending=False).head(top_n)\n",
                "    \n",
                "    return keywords_df\n",
                "\n",
                "print(\"‚úì IndoBERT keyword extraction functions defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract keywords using IndoBERT\n",
                "print(\"=\" * 80)\n",
                "print(\"KEYWORD EXTRACTION USING INDOBERT\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for sentiment in ['positive', 'neutral', 'negative']:\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"Top Keywords (IndoBERT) - {sentiment.upper()}\")\n",
                "    print(f\"{'='*80}\")\n",
                "    \n",
                "    texts = df[df['sentiment_roberta'] == sentiment]['preprocessed'].tolist()\n",
                "    \n",
                "    if len(texts) > 0:\n",
                "        keywords = extract_keywords_bert(texts, top_n=20, sample_size=1000)\n",
                "        print(keywords.to_string(index=False))\n",
                "        \n",
                "        # Visualize\n",
                "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "        \n",
                "        # BERT score\n",
                "        axes[0].barh(keywords['keyword'], keywords['bert_score'], \n",
                "                    color=colors[sentiment], edgecolor='black', alpha=0.8)\n",
                "        axes[0].set_xlabel('BERT Similarity Score', fontsize=12)\n",
                "        axes[0].set_ylabel('Keyword', fontsize=12)\n",
                "        axes[0].set_title(f'Top 20 Keywords (IndoBERT) - {sentiment.capitalize()}', \n",
                "                         fontsize=14, fontweight='bold')\n",
                "        axes[0].invert_yaxis()\n",
                "        axes[0].grid(axis='x', alpha=0.3)\n",
                "        \n",
                "        # Frequency\n",
                "        axes[1].barh(keywords['keyword'], keywords['frequency'], \n",
                "                    color=colors[sentiment], edgecolor='black', alpha=0.6)\n",
                "        axes[1].set_xlabel('Frequency', fontsize=12)\n",
                "        axes[1].set_ylabel('Keyword', fontsize=12)\n",
                "        axes[1].set_title(f'Keyword Frequency - {sentiment.capitalize()}', \n",
                "                         fontsize=14, fontweight='bold')\n",
                "        axes[1].invert_yaxis()\n",
                "        axes[1].grid(axis='x', alpha=0.3)\n",
                "        \n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "    else:\n",
                "        print(f\"No data for {sentiment} sentiment\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ WordCloud Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"GENERATING WORDCLOUDS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
                "\n",
                "for idx, sentiment in enumerate(['positive', 'neutral', 'negative']):\n",
                "    print(f\"\\nGenerating WordCloud for {sentiment}...\")\n",
                "    \n",
                "    texts = ' '.join(df[df['sentiment_roberta'] == sentiment]['preprocessed'].tolist())\n",
                "    \n",
                "    if len(texts) > 0:\n",
                "        wordcloud = WordCloud(\n",
                "            width=800, \n",
                "            height=400,\n",
                "            background_color='white',\n",
                "            colormap='Greens' if sentiment == 'positive' else ('Oranges' if sentiment == 'neutral' else 'Reds'),\n",
                "            max_words=100,\n",
                "            relative_scaling=0.5,\n",
                "            min_font_size=10\n",
                "        ).generate(texts)\n",
                "        \n",
                "        axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
                "        axes[idx].set_title(f'{sentiment.capitalize()} Sentiment', fontsize=16, fontweight='bold')\n",
                "        axes[idx].axis('off')\n",
                "    else:\n",
                "        axes[idx].text(0.5, 0.5, f'No data for {sentiment}', \n",
                "                      ha='center', va='center', fontsize=14)\n",
                "        axes[idx].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úì WordClouds generated!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Export Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"EXPORTING RESULTS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Select columns to export\n",
                "export_df = df[[\n",
                "    'original_text',\n",
                "    'preprocessed',\n",
                "    'sentiment_roberta',\n",
                "    'sentiment_score_roberta',\n",
                "    'source'\n",
                "]].copy()\n",
                "\n",
                "# Rename columns\n",
                "export_df.columns = ['text', 'cleaned_text', 'sentiment', 'sentiment_score', 'source']\n",
                "\n",
                "# Export to CSV\n",
                "output_file = DATA_PATH + 'CoreTax_Sentiment_Analysis_Results.csv'\n",
                "export_df.to_csv(output_file, index=False, encoding='utf-8')\n",
                "\n",
                "print(f\"\\n‚úì Results exported to: {output_file}\")\n",
                "print(f\"Total rows exported: {len(export_df):,}\")\n",
                "\n",
                "# Download file (for Google Colab)\n",
                "try:\n",
                "    from google.colab import files\n",
                "    print(f\"\\nDownloading file...\")\n",
                "    files.download(output_file)\n",
                "    print(f\"‚úì File downloaded!\")\n",
                "except:\n",
                "    print(f\"\\n(File saved to Google Drive: {output_file})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"SUMMARY STATISTICS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(f\"\\nüìä OVERALL STATISTICS\")\n",
                "print(f\"  Total data analyzed: {len(df):,} rows\")\n",
                "print(f\"  Data sources: {df['source'].nunique()}\")\n",
                "print(f\"  Average text length (original): {df['original_length'].mean():.2f} characters\")\n",
                "print(f\"  Average text length (preprocessed): {df['preprocessed_length'].mean():.2f} characters\")\n",
                "\n",
                "print(f\"\\nüòä SENTIMENT DISTRIBUTION\")\n",
                "for sentiment in ['positive', 'neutral', 'negative']:\n",
                "    count = len(df[df['sentiment_roberta'] == sentiment])\n",
                "    percentage = (count / len(df)) * 100\n",
                "    avg_score = df[df['sentiment_roberta'] == sentiment]['sentiment_score_roberta'].mean()\n",
                "    print(f\"  {sentiment.capitalize():8s}: {count:6,} ({percentage:5.2f}%) | Avg confidence: {avg_score:.4f}\")\n",
                "\n",
                "print(f\"\\nüì± DISTRIBUTION BY SOURCE\")\n",
                "for source in df['source'].unique():\n",
                "    count = len(df[df['source'] == source])\n",
                "    percentage = (count / len(df)) * 100\n",
                "    print(f\"  {source:12s}: {count:6,} ({percentage:5.2f}%)\")\n",
                "\n",
                "print(f\"\\nüéØ SENTIMENT BY SOURCE\")\n",
                "print(pd.crosstab(df['source'], df['sentiment_roberta'], margins=True))\n",
                "\n",
                "print(f\"\\n‚úì Analysis complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}